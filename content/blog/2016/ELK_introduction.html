---
title: "Using E(Elasticsearch)L(Logstash)K(Kibana) for indexing,searching and analyzing application log data"
created: !!timestamp '2016-03-25 07:23:00'
image: /media/images/blog/2016/ELK.jpg
tags:
    - Data
---
{% block excerpt %}
{% mark excerpt %}
Nowadays,all kinds of applications and systems logs are very valuable for us,we can analysis the logs get valuable informations and make the right business decisions.But usually the logs distributed here and there across the system and the content of logs is usually unstructured,so its quite diffcult to analyze logs.

[Elasticsearch]() along with [Logstash]() and [Kibana]() are both excellent products of [elastic](),they can use them together to help us index,search and analyze our data.
{% endmark %}
{% endblock %}

#Logstatsh
Logstatsh is a data pipeline that can efficiently process a growing list of log,event and other unstructred data source realtime, and route the formatted data to various downstreams.you can route the data to Elasticsearch,so the data can be used to do analysis. you can also store the data to MongoDB or archiving data to HDFS or S3. by installing some plugins you can also sent these data to monitoring system like Datadog.
<figure class="illustration img-thumbnail"  style="width: 60%;">
    <img src="/media/images/blog/2016/logstash_image.png">
</figure>

###installing
to install Logstash, Java7 or later required. you can download the package from [here](https://www.elastic.co/downloads/logstash),after download package and unzipp it you can run below comands to test if Logstash install successfully.
{% syntax shell %}
cd logstash-{your version}
bin/logstash -e 'input { stdin { } } output { stdout {} }' 
# -e enable to specify a configuration directly from the command line 
# This pipeline takes input from the standard input and move that input to standard output
{% endsyntax %}
if installed successfully , you can type `hello world` and you should see below output
{% syntax shell %}
hello world
2013-11-21T01:22:14.405+0000 0.0.0.0 hello world
{% endsyntax %}

###Pipeline
A Logstash pipeline in most use cases has one or more input and output and filter plugins ,the Logstash configuration file defines the `Logstash pipeline`.When you start a Logstash instance , use `-f {path/to/file}` option to specify the configuration file that defines that instance's pipeline , the `input` and `output` filter is two required elements, and the `filter` is the optional element
<figure class="illustration img-thumbnail"  style="width: 60%;">
    <img src="/media/images/blog/2016/basic_logstash_pipeline.png">
</figure>

[input plugins](https://www.elastic.co/guide/en/logstash/current/input-plugins.html) enables a special source of events to be read by Logstash.
[output plugin](https://www.elastic.co/guide/en/logstash/current/output-plugins.html) sends event data to a particular destination.
[filter plugin](https://www.elastic.co/guide/en/logstash/current/filter-plugins.html) performs intermediary processing on an event. 

lets take a example about using Logstash pipeline to anaylyze the nginx access log , you need to create the configuration file for plugins , for example `access_log.conf`.

and in `access_log.conf` , you can add configurations about pipeline. But we need to tell how to parse our logs, and this part we need add a 
pattern file
{% syntax shell %}
$ mkdir /etc/logstash/patterns
$ vi /etc/logstash/patterns/nginx
NGINX_ACCESS %{IPORHOST:clientip} %{NGUSER:ident} %{NGUSER:auth} \[%{HTTPDATE:timestamp}\] "%{WORD:verb} %{URIPATHPARAM:request} HTTP/%{NUMBER:httpversion}" %{NUMBER:response} (?:%{NUMBER:bytes}|-) (?:"(?:%{URI:referrer}|-)"|%{QS:referrer}) %{QS:agent}
{% endsyntax %}

{% syntax shell %}
input {
    file {
        type => "nginx_access"
        path => "/path/to/your/access/log"
        start_position => beginning 
        ignore_older => 0 
    }
}
filter {
	if [type] == "nginx_access" {
	    grok {
	        patterns_dir => "/etc/logstash/patterns"
	        match => { "message" => "%{NGINX_ACCESS}"} 
	        #you can add additional field by 'add_field',or remove field by 'remove_field'
	    }
	    geoip {
	        	source => "clientip"
	     }
    }
}
output {
    elasticsearch {
    }
}
{% endsyntax %}
the above Logstash use `/path/to/your/access/log` as the input . and in order to process the entire file , we need to set `start_position` beginning, otherwise it will only monitor for new information , like unix `tail -f` command. and the default behavior of file input plugin will ignore information whose last modification is greater than 86400s, so my files much older ,so I not ignore these informations.

we use [grok](https://www.elastic.co/guide/en/logstash/current/plugins-filters-grok.html) as the filter plugin,it can easily parse arbitrary text and structure it. in order to structure nginx acess log , we need provide a pattern file for grok to use ,and then use the pattern to parse the input, after processing we have structure data like below
{% syntax shell %}
{
"clientip" : "83.149.9.216",
"ident" : ,
"auth" : ,
"timestamp" : "04/Jan/2015:05:13:42 +0000",
"verb" : "GET",
"request" : "/presentations/logstash-monitorama-2013/images/kibana-search.png",
"httpversion" : "HTTP/1.1",
"response" : "200",
"bytes" : "203023",
"referrer" : "http://semicomplete.com/presentations/logstash-monitorama-2013/",
"agent" : "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36"
}
{% endsyntax %}

now the logs has broken to specific fields ,we can configure to index the data into an Elasticsearch cluster
{% syntax shell %}
output {
    elasticsearch {
    }
}
{% endsyntax %}
the above cofiguration assuems Logstash and Elasticsearch to be running  on the same instance , you can specify a remote Elasticsearch instance using `hosts` configuration like `hosts => "es-machine:9092"`.Logstash will use http protocol to connect Elasticsearch, and you can also define which index to write events to ,the default value is "logstash-%{+YYYY.MM.dd}"
 



And by now we have finish the basic configuration of Logstash, it will process log file from `input` parse each line of log and route the structured data to Elasticsearch. you can verify the configuration , running below comannds
{% syntax shell %}
bin/logstash -f first-pipeline.conf --configtest   #configtest means just test if configurations have any errors
{% endsyntax %}
and you can start the Logstash by run following commands
{% syntax shell %}
bin/logstash -f first-pipeline.conf
{% endsyntax %}

###File beat 
By now we just process a single file from the same server with Logstash,what about we want process other server's logs? [filebeat](https://github.com/elastic/beats/tree/master/filebeat) is just the tool to solve this problem.

The filebeat client is a lightweight, resource-friendly tool that collects logs from files on the server and forwards these logs to your Logstash instance for processing. The Filebeat client uses the secure Beats protocol to communicate with your Logstash instance.
Default Logstash configuration includes the Beats input plugin, which is designed to be resource-friendly. To install Filebeat on your data source machine, download the appropriate package from the Filebeat [product page](https://www.elastic.co/downloads/beats/filebeat).
edit `filebeat.yml` configuration file
{% syntax shell %}
filebeat:
  prospectors:
    -
      paths:
        - "/path/to/log"       # the log path
      fields:
        type: syslog
output:
  logstash:
    hosts: ["localhost:5043"]	# usually , filebeat and logstash run at different server
  tls:
    certificate: /path/to/ssl-certificate.crt 		#path to the ssl certificate for the Logstash instancce.
    certificate_key: /path/to/ssl-certificate.key
    certificate_authorities: /path/to/ssl-certificate.crt
    timeout: 15
{% endsyntax %}
and now we can configure the Logstash confgurations to use Filebeat input plugin in our `access_log.conf`
add below lines in `input` section
{% syntax shell %}
beats {
    port => "5043"
    ssl => true
    ssl_certificate => "/path/to/ssl-cert" 	#path to the ssl certificate that the Logstash instance use to authenticate itself to Filebeat
    ssl_key => "/path/to/ssl-key" 			# path to the key for ssl certificate
}
{% endsyntax %}

#ElasticSearch
ElasticSearch is a highly scalable open-source full-text search and analytics engine.It allows your to store ,search and analyze big volumes of data quickly in near real time(slightly latency from the time you index the document untile it becomes searchable).It can be used as the underlying engine/technology that powers applications that have complex search features and requirements.

-   you can use ElasticSearch as the storage of your application and provide search/autocomplete suggestion for them.
-   you can collect the log from all your servers and analyze the data to look for the trends statistics, summarizations.
-   you can even use it as alert platform which  will sent alert message when expect thing happens.

-   `cluster` is the collection of one or more nodes(servers) that together holds the server and provide indexing and searching across all nodes ,each one should have a unique cluster name(default is elasticsearch).
-   `node` is the single server that part of `cluster` ,store the data and participate in cluster's index,search capabilities, a node also need a name(default will random Marvel character name) , you can configure which cluster a node belongs to(by default it belongs to elasticsearch)
-   `index` is the collection of data from the same source , for example there's a index for your nginx access log and another index for system logs
-   `Document` is the basic unit of the index, for example a line in access log
-   `Shards and Replicas` if your have too many Document in the index, you can shard the the index into multiple pieces called shards , each shard is fully-functional and independent index, and these shards can distribute in different nodes ,thus shards can allow you to horizontally split/scale your content volume , and parallelize operations across shards so can increse the performance , the mechanics of how shards distributed and how its documents are aggreate back is managed by ElasticSearch itself.
if your have too many nodes, there's maybe a failure node anytime, so you need `Replica` , it can provide high availability
each index can be split into multiple shards and be replicated zero(no replicas) or more times. Once replicated,each index will have primary shards(original shards) and replica shards(copied shards), you can define shards and replicas when create the index.

###Installation
you also need Java7 or later to run ElasticSearch, you can just download from [here](https://www.elastic.co/downloads),
after download latest version of Elasticsearch then extract it then you can run below commands to check if install successfully
{% syntax shell %}
cd elasticsearch-{your version}/bin
./elasticsearch
{% endsyntax %}
if everything goes well , you should see below output
{% syntax shell %}
./elasticsearch
[2014-03-13 13:42:17,218][INFO ][node           ] [New Goblin] version[2.2.1], pid[2085], build[5c03844/2014-02-25T15:52:53Z]
[2014-03-13 13:42:17,219][INFO ][node           ] [New Goblin] initializing ...
[2014-03-13 13:42:17,223][INFO ][plugins        ] [New Goblin] loaded [], sites []
[2014-03-13 13:42:19,831][INFO ][node           ] [New Goblin] initialized
[2014-03-13 13:42:19,832][INFO ][node           ] [New Goblin] starting ...
[2014-03-13 13:42:19,958][INFO ][transport      ] [New Goblin] bound_address {inet[/0:0:0:0:0:0:0:0:9300]}, publish_address {inet[/192.168.8.112:9300]}
[2014-03-13 13:42:23,030][INFO ][cluster.service] [New Goblin] new_master [New Goblin][rWMtGj3dQouz2r6ZFL9v4g][mwubuntu1][inet[/192.168.8.112:9300]], reason: zen-disco-join (elected_as_master)
[2014-03-13 13:42:23,100][INFO ][discovery      ] [New Goblin] elasticsearch/rWMtGj3dQouz2r6ZFL9v4g
[2014-03-13 13:42:23,125][INFO ][http           ] [New Goblin] bound_address {inet[/0:0:0:0:0:0:0:0:9200]}, publish_address {inet[/192.168.8.112:9200]}
[2014-03-13 13:42:23,629][INFO ][gateway        ] [New Goblin] recovered [1] indices into cluster_state
[2014-03-13 13:42:23,630][INFO ][node           ] [New Goblin] started
{% endsyntax %}
from log you can see the node `New Goblin` is started and its the master of this single cluster.
and it also start a restful service on port 9200.  

you  can also run the Elastic search as daemon just add `-d` when start it, and also your may want to write the PID of ElasticSearch when its run in backgroud, then you need to add `-p pid` then it will create a file(named pid ,you can also change it to something else) and store the pid in that file ,then later you can kill it by run below commands
{% syntax shell %}
kill `cat pid` 
{% endsyntax %}

###API
once ElasticSearch startup , it will provide restful service on port 9220. you can simply use any tool to test these APIs
{% syntax shell %}
curl 'localhost:9200/_cat/health?v'  #cluster health.
epoch      timestamp cluster       status node.total node.data shards pri relo init unassign
1394735289 14:28:09  elasticsearch green           1         1      0   0    0    0        0


curl 'localhost:9200/_cat/nodes?v'  #nodes 
host         ip        heap.percent ram.percent load node.role master name
mwubuntu1    127.0.1.1            8           4 0.00 d         *      New Goblin


curl -XPUT 'localhost:9200/customer?pretty'  #create a index
{
  "acknowledged" : true
}

curl 'localhost:9200/_cat/indices?v'        # indices
health index    pri rep docs.count docs.deleted store.size pri.store.size
yellow customer   5   1          0            0       495b           495b
{% endsyntax %}

and there're also APIs can put the document in the index or [search](https://www.elastic.co/guide/en/elasticsearch/reference/current/search.html) APIs, fortunately we don't call these API ourself most of case , we can use `Logstash` to create index and document and use `Kibana` to search data for us.

#Kibana
[Kibana](https://www.elastic.co/products/kibana) is a analytics and visualization platform , and it can be integrate with `Elasticsearch` Seamless
, and provide sophisticated analytics, and its very easy to setup.

###Installation
to get Kibana, run below commands

-   download the [kibana](https://www.elastic.co/downloads/kibana)
-   Extract the .zip or .tar.gz archive file
-   you can edit <em>config/kibana.yml</em> if you want set <em>elasticsearch.url</em>(you can ignore this if you Elasticsearch run on same server with default port)
-   run kibana from install directory : bin/kibana,and after it start up,you can open localhost:5601 in your broswer.


###Index Patterns
Index pattern is a string with optional wildcards that can match multiple indices.For example in comman logging use case, a typical index name contains the date in MM-DD-YYYY format. Open `localhost:5601` in browser , click the `setting` tab ,then `indcies` tab , click `Add New` to define a new index pattern. and since the default index logstash write to ElasticSearch is "logstash-%{+YYYY.MM.dd}" , so you can  input logstash-* as the index pattern,and make sure the `index contains time-based events` is checked ,since our log is time-based.

###Discover Data
After index pattern defined, you click `Discover` tab,and you should see something like below pic
<figure class="illustration img-thumbnail"  style="width: 60%;">
    <img src="/media/images/blog/2016/kibana_discover.png">
</figure>
there's a search box that enables you to search , you can check [query syntax](http://www.elastic.co/guide/en/elasticsearch/reference/current//query-dsl-query-string-query.html#query-string-syntax) to learn more about how to search.
and you can also select which index you want to search. you can construct the searchs by using the filed and the values your are interested in , and you can also narrow the display to only specific fields of interests as show in below pics,so if you have a transactionId in your system, you don't need go to server to grep related log ,just search in kibina, and you will get all the releated messages.
<figure class="illustration img-thumbnail"  style="width: 60%;">
    <img src="/media/images/blog/2016/kibana_search.png">
</figure>

###Visualize
besides search data , you can visualize you data using visulize tab, click Visualize tab to start.
Visulizations depend on Elasticsearch aggregations in two different types :`bucket` aggregation and `metric` aggregations.
A bucket aggregation sorts your data according to criteria you specify.For example the timestamp in access log. there's a very detailed [tutorial](https://www.elastic.co/guide/en/kibana/current/tutorial-visualizing.html) in elastic about the visualize, you can check it out.

<figure class="illustration img-thumbnail"  style="width: 60%;">
    <img src="/media/images/blog/2016/kibana_visulization.png">
</figure>

###Dashbord
After you saved your visualizes, you can then add these visulizes to your dashboard , the process like below .
you can customizing dashboards elements very easily ,and you can also add some configuration to automatically refreshing the page, you can check [offical document](https://www.elastic.co/guide/en/kibana/current/dashboard.html) for more information.

<figure class="illustration img-thumbnail"  style="width: 60%;">
    <img src="/media/images/blog/2016/kibana_dashboard.gif">
</figure>







